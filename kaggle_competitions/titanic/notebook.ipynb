{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b706044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e597b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, input_dim, output_dim, name = 'LinearRegression'):\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.weights = np.random.rand(input_dim, self.output_dim)\n",
    "        self.bias = np.random.rand(1, self.output_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        p = self._predict(x)\n",
    "        return (p >= 0.5).astype(int)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.name}({self.input_dim}, {self.output_dim})'\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \n",
    "    def train(self, x, y, epoch = 1000, learning_rate = 1e-1):\n",
    "        num_samples = x.shape[0]\n",
    "        last_loss = np.inf\n",
    "\n",
    "        for i in range(epoch):\n",
    "            # Step 1: Predict\n",
    "            p = self._predict(x)\n",
    "\n",
    "            # Step 2: Calculate errors\n",
    "            e = p - y\n",
    "\n",
    "            # Calculate gradients.\n",
    "            c = 2 / num_samples\n",
    "            grad_w = c * (x.T @ e)\n",
    "            grad_b = c * np.sum(e, axis = 0, keepdims = True)\n",
    "\n",
    "            # Update params.\n",
    "            self.weights -= learning_rate * grad_w\n",
    "            self.bias -= learning_rate * grad_b\n",
    "            \n",
    "            loss = np.sum(e ** 2) / num_samples\n",
    "\n",
    "            # If it does not improve, halt the training process.\n",
    "            if (last_loss - loss) == 0:\n",
    "                print(f'({i}/{epoch}) Function is optimized, loss was not improved. Done.')\n",
    "                return\n",
    "            else:\n",
    "                last_loss = loss\n",
    "        \n",
    "            # Always print the first and the last iteration.\n",
    "            if i % 10 == 0 or i == epoch - 1:\n",
    "                print(f'({i}/{epoch}) Loss {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "628dd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./train.csv')\n",
    "train, test = train_test_split(dataset, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783389e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cabin(value):\n",
    "    if value is np.nan:\n",
    "        return 0\n",
    "    if len(value.split(' ')) != 1:\n",
    "        return -1\n",
    "    letter = value[0]\n",
    "    number = int(value[1:] or '0')\n",
    "    return (ord(letter) - 64) * 1000 + number\n",
    "\n",
    "\n",
    "def clean_data(dataset):\n",
    "    dataset = dataset.copy()\n",
    "    drop_columns =  ['Name', 'Ticket', 'Pclass', 'PassengerId', 'Cabin']\n",
    "    feature_columns = ['Age', 'Fare', 'Sex', 'Parch', 'SibSp', 'E_C', 'E_Q', 'E_S']\n",
    "\n",
    "    dataset.Age = dataset.Age.fillna(-1) / 1000\n",
    "    dataset.Fare = dataset.Fare / 1000\n",
    "    dataset.Sex = dataset.Sex.map(lambda sex: {'male': 1, 'female': 0}[sex])\n",
    "    dataset.Cabin = dataset.Cabin.map(clean_cabin)\n",
    "    \n",
    "    # Embarked\n",
    "    dataset.Embarked = dataset.Embarked.fillna(dataset['Embarked'].mode()[0])\n",
    "    dataset = pd.get_dummies(dataset, columns=['Embarked'], prefix='E', dtype=int)\n",
    "\n",
    "    return dataset[feature_columns].to_numpy(), dataset[['Survived']].to_numpy()\n",
    "\n",
    "x_train, y_train = clean_data(train)\n",
    "x_test, y_test = clean_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1115ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/1000) Loss 2.73301908\n",
      "(10/1000) Loss 0.20170392\n",
      "(20/1000) Loss 0.17488365\n",
      "(30/1000) Loss 0.16520090\n",
      "(40/1000) Loss 0.16088853\n",
      "(50/1000) Loss 0.15875398\n",
      "(60/1000) Loss 0.15762271\n",
      "(70/1000) Loss 0.15699527\n",
      "(80/1000) Loss 0.15663613\n",
      "(90/1000) Loss 0.15642554\n",
      "(100/1000) Loss 0.15629933\n",
      "(110/1000) Loss 0.15622188\n",
      "(120/1000) Loss 0.15617293\n",
      "(130/1000) Loss 0.15614082\n",
      "(140/1000) Loss 0.15611872\n",
      "(150/1000) Loss 0.15610261\n",
      "(160/1000) Loss 0.15609013\n",
      "(170/1000) Loss 0.15607986\n",
      "(180/1000) Loss 0.15607094\n",
      "(190/1000) Loss 0.15606287\n",
      "(200/1000) Loss 0.15605534\n",
      "(210/1000) Loss 0.15604815\n",
      "(220/1000) Loss 0.15604121\n",
      "(230/1000) Loss 0.15603442\n",
      "(240/1000) Loss 0.15602776\n",
      "(250/1000) Loss 0.15602120\n",
      "(260/1000) Loss 0.15601472\n",
      "(270/1000) Loss 0.15600830\n",
      "(280/1000) Loss 0.15600195\n",
      "(290/1000) Loss 0.15599565\n",
      "(300/1000) Loss 0.15598941\n",
      "(310/1000) Loss 0.15598323\n",
      "(320/1000) Loss 0.15597710\n",
      "(330/1000) Loss 0.15597101\n",
      "(340/1000) Loss 0.15596498\n",
      "(350/1000) Loss 0.15595900\n",
      "(360/1000) Loss 0.15595307\n",
      "(370/1000) Loss 0.15594718\n",
      "(380/1000) Loss 0.15594135\n",
      "(390/1000) Loss 0.15593556\n",
      "(400/1000) Loss 0.15592981\n",
      "(410/1000) Loss 0.15592412\n",
      "(420/1000) Loss 0.15591847\n",
      "(430/1000) Loss 0.15591287\n",
      "(440/1000) Loss 0.15590731\n",
      "(450/1000) Loss 0.15590180\n",
      "(460/1000) Loss 0.15589633\n",
      "(470/1000) Loss 0.15589091\n",
      "(480/1000) Loss 0.15588553\n",
      "(490/1000) Loss 0.15588019\n",
      "(500/1000) Loss 0.15587490\n",
      "(510/1000) Loss 0.15586964\n",
      "(520/1000) Loss 0.15586444\n",
      "(530/1000) Loss 0.15585927\n",
      "(540/1000) Loss 0.15585414\n",
      "(550/1000) Loss 0.15584906\n",
      "(560/1000) Loss 0.15584402\n",
      "(570/1000) Loss 0.15583901\n",
      "(580/1000) Loss 0.15583405\n",
      "(590/1000) Loss 0.15582913\n",
      "(600/1000) Loss 0.15582424\n",
      "(610/1000) Loss 0.15581940\n",
      "(620/1000) Loss 0.15581459\n",
      "(630/1000) Loss 0.15580982\n",
      "(640/1000) Loss 0.15580509\n",
      "(650/1000) Loss 0.15580040\n",
      "(660/1000) Loss 0.15579575\n",
      "(670/1000) Loss 0.15579113\n",
      "(680/1000) Loss 0.15578655\n",
      "(690/1000) Loss 0.15578200\n",
      "(700/1000) Loss 0.15577749\n",
      "(710/1000) Loss 0.15577302\n",
      "(720/1000) Loss 0.15576858\n",
      "(730/1000) Loss 0.15576417\n",
      "(740/1000) Loss 0.15575980\n",
      "(750/1000) Loss 0.15575547\n",
      "(760/1000) Loss 0.15575117\n",
      "(770/1000) Loss 0.15574690\n",
      "(780/1000) Loss 0.15574267\n",
      "(790/1000) Loss 0.15573846\n",
      "(800/1000) Loss 0.15573430\n",
      "(810/1000) Loss 0.15573016\n",
      "(820/1000) Loss 0.15572606\n",
      "(830/1000) Loss 0.15572199\n",
      "(840/1000) Loss 0.15571795\n",
      "(850/1000) Loss 0.15571394\n",
      "(860/1000) Loss 0.15570996\n",
      "(870/1000) Loss 0.15570601\n",
      "(880/1000) Loss 0.15570210\n",
      "(890/1000) Loss 0.15569821\n",
      "(900/1000) Loss 0.15569436\n",
      "(910/1000) Loss 0.15569053\n",
      "(920/1000) Loss 0.15568673\n",
      "(930/1000) Loss 0.15568297\n",
      "(940/1000) Loss 0.15567923\n",
      "(950/1000) Loss 0.15567552\n",
      "(960/1000) Loss 0.15567184\n",
      "(970/1000) Loss 0.15566818\n",
      "(980/1000) Loss 0.15566456\n",
      "(990/1000) Loss 0.15566096\n",
      "(999/1000) Loss 0.15565774\n",
      "20.670391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9x/jyl6_jcn10g17ynbhk1t47wm0000gn/T/ipykernel_35624/905470519.py:18: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return x @ self.weights + self.bias\n",
      "/var/folders/9x/jyl6_jcn10g17ynbhk1t47wm0000gn/T/ipykernel_35624/905470519.py:18: RuntimeWarning: overflow encountered in matmul\n",
      "  return x @ self.weights + self.bias\n",
      "/var/folders/9x/jyl6_jcn10g17ynbhk1t47wm0000gn/T/ipykernel_35624/905470519.py:18: RuntimeWarning: invalid value encountered in matmul\n",
      "  return x @ self.weights + self.bias\n",
      "/var/folders/9x/jyl6_jcn10g17ynbhk1t47wm0000gn/T/ipykernel_35624/905470519.py:33: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad_w = c * (x.T @ e)\n",
      "/var/folders/9x/jyl6_jcn10g17ynbhk1t47wm0000gn/T/ipykernel_35624/905470519.py:33: RuntimeWarning: overflow encountered in matmul\n",
      "  grad_w = c * (x.T @ e)\n",
      "/var/folders/9x/jyl6_jcn10g17ynbhk1t47wm0000gn/T/ipykernel_35624/905470519.py:33: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad_w = c * (x.T @ e)\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(input_dim = x_train.shape[1], output_dim = 1)\n",
    "model.train(x_train, y_train)\n",
    "\n",
    "error = (model(x_test) - y_test) ** 2\n",
    "error = error.sum() / len(error)\n",
    "error = error * 100\n",
    "print(f'{error:0f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
